{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "curious-commercial",
   "metadata": {},
   "source": [
    "# Word Window Classification\n",
    "\n",
    "我们现在需要尝试解决一个NLP的任务示例，下面是我们会学习到的：\n",
    "\n",
    "    1.数据:创建一个批量张量的数据集\n",
    "    2.模型搭建\n",
    "    3.模型训练\n",
    "    4.预测\n",
    "    \n",
    "这一部分，我们的目标要训练一个模型，使得这个模型可以找到一个句子中与```LOCATION```有关的单词（这个单词只有一个词，而不能是词组）。这个任务称为```Word Window Classification```。我们不想让我们的模型在每次前向传播中只查看一个单词，我们希望它能够考虑相关单词的上下文。也就是说，对于每个单词，我们希望我们的模型能够注意到周围的单词。就让我们一探究竟吧!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-reference",
   "metadata": {},
   "source": [
    "## 1. Data\n",
    "任何机器学习项目的第一个任务都是建立我们的训练集。通常，都会有一个我们要使用的训练语料库。在NLP任务中，语料库通常是```.txt```或```.csv```文件，其中每一行对应一个句子或一个表格数据点。在我们的简单任务中，我们假定已经将数据和相应的标签读入了Python列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "limiting-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our raw data, which consists of sentences\n",
    "corpus = [\n",
    "          \"We always come to Paris\",\n",
    "          \"The professor is from Australia\",\n",
    "          \"I live in Stanford\",\n",
    "          \"He comes from Taiwan\",\n",
    "          \"The capital of Turkey is Ankara\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-version",
   "metadata": {},
   "source": [
    "### 1.1 预处理\n",
    "为了使我们的模型更容易学习，我们通常对我们的数据应用一些预处理步骤。这在处理文本数据时尤为重要。下面是一些文本预处理的例子:\n",
    "* 标记化:将句子标记成单词。\n",
    "* 小写:将所有字母改为小写。\n",
    "* 去除噪声:去除特殊字符(如标点)。\n",
    "* 停止词删除:删除常用的词。\n",
    "哪些预处理步骤是必要的是由手头的任务决定的。例如，虽然在某些任务中删除特殊字符是有用的，但在其他任务中，它们可能是重要的(例如，如果我们要处理多种语言)。对于我们的任务，我们将单词小写化并标记化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "general-republican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['we', 'always', 'come', 'to', 'paris'],\n",
       " ['the', 'professor', 'is', 'from', 'australia'],\n",
       " ['i', 'live', 'in', 'stanford'],\n",
       " ['he', 'comes', 'from', 'taiwan'],\n",
       " ['the', 'capital', 'of', 'turkey', 'is', 'ankara']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our function is a simple one, we lowercase the letters\n",
    "# and then tokenize the words.\n",
    "def preprocess_sentence(sentence):\n",
    "  return sentence.lower().split()\n",
    "\n",
    "# Create our training set\n",
    "train_sentences = [sent.lower().split() for sent in corpus]\n",
    "train_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-visit",
   "metadata": {},
   "source": [
    "对于每个训练示例，我们都应该有相应的标签。回想一下，我们模型的目标是确定哪些单词对应于一个```LOCATION```。也就是说，我们希望我们的模型为所有不是```LOCATION```的单词输出0，为```LOCATION```的单词输出1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eligible-bouquet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1, 0, 1]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set of locations that appear in our corpus\n",
    "locations = set([\"australia\", \"ankara\", \"paris\", \"stanford\", \"taiwan\", \"turkey\"])\n",
    "\n",
    "# Our train labels\n",
    "train_labels = [[1 if word in locations else 0 for word in sent] for sent in train_sentences]\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-aurora",
   "metadata": {},
   "source": [
    "### 1.2 把单词转换为词向量\n",
    "让我们更仔细地看看我们的训练数据。我们拥有的每个数据点都是一个单词序列。另一方面，我们知道机器学习模型是用向量中的数字来工作的。我们如何将文字转化为数字?你可能在想词嵌入，你是对的!\n",
    "\n",
    "假设我们有一个内嵌查找表E，其中每一行对应一个内嵌。也就是说，我们词汇表中的每个单词在这个表中都有一个对应的嵌入行i。每当我们想要找到一个单词的嵌入，我们将遵循以下步骤:\n",
    "\n",
    "    1. 在嵌入表中找到单词对应的索引i: word->index\n",
    "    2. 索引到嵌入表中，得到嵌入:Index ->embedding\n",
    "    \n",
    "让我们看看第一步。我们应该将我们词汇表中的所有单词分配到相应的索引中。我们可以这样做:\n",
    "\n",
    "    1. 找出语料库中所有独特的单词。\n",
    "    2. 给每一个都指定一个索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "unlikely-silly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'always',\n",
       " 'ankara',\n",
       " 'australia',\n",
       " 'capital',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'from',\n",
       " 'he',\n",
       " 'i',\n",
       " 'in',\n",
       " 'is',\n",
       " 'live',\n",
       " 'of',\n",
       " 'paris',\n",
       " 'professor',\n",
       " 'stanford',\n",
       " 'taiwan',\n",
       " 'the',\n",
       " 'to',\n",
       " 'turkey',\n",
       " 'we'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all the unique words in our corpus \n",
    "vocabulary = set(w for s in train_sentences for w in s)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-pharmacology",
   "metadata": {},
   "source": [
    "词汇库现在包含了我们语料库中的所有单词。另一方面，在测试模型期间，会出现我们的词汇表中没有包含的单词。如果我们能找到一种方法来表示未知单词，我们的模型仍然可以推断出它们是否是一个```LOCATION```，因为我们也会查看每个预测的邻近单词。\n",
    "\n",
    "我们引入了一个特殊的标记<unk>，来处理词汇表之外的单词。如果需要，我们可以选择另一个字符串作为未知标记。这里唯一的要求是我们的标志应该是唯一的:我们应该只使用这个标志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "legendary-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the unknown token to our vocabulary\n",
    "vocabulary.add(\"<unk>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-identifier",
   "metadata": {},
   "source": [
    "前面我们提到过，我们的任务被称为词窗口分类，因为我们的模型在需要做出预测时，除了查看给定的单词外，还查看周围的单词。\n",
    "\n",
    "例如，让我们以“We always come to Paris”这句话为例。这个句子对应的训练标号是```0,0,0,0,1```，因为只有最后一个单词Paris是一个位置。在一次传递中(即调用forward())，我们的模型将尝试为一个单词生成正确的标签。假设我们的模型试图为Paris生成正确的标签1。如果我们只让我们的模型去观测Paris，我们就会错过重要的信息，比如to这个词经常和location一起出现。\n",
    "\n",
    "单词窗口允许我们的模型在进行预测时考虑每个单词周围的+N或-N个单词。在我们前面关于Paris的例子中，如果我们有一个窗口大小为1，这意味着我们的模型将查看紧接在Paris之前和之后的单词，它们是to，还有，emm, 什么都没有。现在，这又引出了另一个问题。Paris在句子的末尾，所以后面没有其他单词。记住，我们在初始化PyTorch模型时定义了它们的输入维数。如果我们将窗口大小设置为1，这意味着我们的模型将在每次传递中接受3个单词。我们不能让我们的模型在一次训练中只输入两个词。\n",
    "\n",
    "解决方案是引入一个特殊的标记，例如```<pad>```，它将被添加到我们的句子中，以确保每个单词周围都有一个有效的窗口。类似于```<unk>```令牌，如果我们愿意，我们可以选择另一个字符串作为我们的pad令牌，只要我们确保它用于唯一的目的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "demographic-winter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<pad>', 'we', 'always', 'come', 'to', 'paris', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the <pad> token to our vocabulary\n",
    "vocabulary.add(\"<pad>\")\n",
    "\n",
    "# Function that pads the given sentence\n",
    "# We are introducing this function here as an example\n",
    "# We will be utilizing it later in the tutorial\n",
    "def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "  window = [pad_token] * window_size\n",
    "  return window + sentence + window\n",
    "\n",
    "# Show padding example\n",
    "window_size = 2\n",
    "pad_window(train_sentences[0], window_size=window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-proposition",
   "metadata": {},
   "source": [
    "现在我们的词汇表已经准备好了，让我们为每个单词指定一个索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "secure-elimination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " 'always': 2,\n",
       " 'ankara': 3,\n",
       " 'australia': 4,\n",
       " 'capital': 5,\n",
       " 'come': 6,\n",
       " 'comes': 7,\n",
       " 'from': 8,\n",
       " 'he': 9,\n",
       " 'i': 10,\n",
       " 'in': 11,\n",
       " 'is': 12,\n",
       " 'live': 13,\n",
       " 'of': 14,\n",
       " 'paris': 15,\n",
       " 'professor': 16,\n",
       " 'stanford': 17,\n",
       " 'taiwan': 18,\n",
       " 'the': 19,\n",
       " 'to': 20,\n",
       " 'turkey': 21,\n",
       " 'we': 22}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are just converting our vocabularly to a list to be able to index into it\n",
    "# Sorting is not necessary, we sort to show an ordered word_to_ind dictionary\n",
    "# That being said, we will see that having the index for the padding token\n",
    "# be 0 is convenient as some PyTorch functions use it as a default value\n",
    "# such as nn.utils.rnn.pad_sequence, which we will cover in a bit\n",
    "ix_to_word = sorted(list(vocabulary))\n",
    "\n",
    "# Creating a dictionary to find the index of a given word\n",
    "word_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-bookmark",
   "metadata": {},
   "source": [
    "太棒了!我们已经准备好将我们的训练句子转换为对应于每个标记的索引序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "desirable-salon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is: ['we', 'always', 'come', 'to', 'kuwait']\n",
      "Going from words to indices: [22, 2, 6, 20, 1]\n",
      "Going from indices to words: ['we', 'always', 'come', 'to', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "# Given a sentence of tokens, return the corresponding indices\n",
    "def convert_token_to_indices(sentence, word_to_ix):\n",
    "  indices = []\n",
    "  for token in sentence:\n",
    "    # Check if the token is in our vocabularly. If it is, get it's index. \n",
    "    # If not, get the index for the unknown token.\n",
    "    if token in word_to_ix:\n",
    "      index = word_to_ix[token]\n",
    "    else:\n",
    "      index = word_to_ix[\"<unk>\"]\n",
    "    indices.append(index)\n",
    "  return indices\n",
    "\n",
    "# More compact version of the same function\n",
    "def _convert_token_to_indices(sentence, word_to_ix):\n",
    "  return [word_to_ind.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
    "\n",
    "# Show an example\n",
    "example_sentence = [\"we\", \"always\", \"come\", \"to\", \"kuwait\"]\n",
    "example_indices = convert_token_to_indices(example_sentence, word_to_ix)\n",
    "restored_example = [ix_to_word[ind] for ind in example_indices]\n",
    "\n",
    "print(f\"Original sentence is: {example_sentence}\")\n",
    "print(f\"Going from words to indices: {example_indices}\")\n",
    "print(f\"Going from indices to words: {restored_example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-finance",
   "metadata": {},
   "source": [
    "在上面的例子中，kuwait显示为<unk>，因为它没有包含在我们的词汇表中。让我们将train_sentences转换为example_padded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unexpected-national",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22, 2, 6, 20, 15],\n",
       " [19, 16, 12, 8, 4],\n",
       " [10, 13, 11, 17],\n",
       " [9, 7, 8, 18],\n",
       " [19, 5, 14, 21, 12, 3]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting our sentences to indices\n",
    "example_padded_indices = [convert_token_to_indices(s, word_to_ix) for s in train_sentences]\n",
    "example_padded_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-commission",
   "metadata": {},
   "source": [
    "现在我们有了词汇表中每个单词的索引，我们可以用```nn.Embedding```在PyTorch中创建一个嵌入表。它是```nn.Embedding(num_words, embedding_dimension)```。其中num_words是词汇表中的单词数，而embeddding_dimension是我们想要的嵌入的维度。\n",
    "```nn.embedding```没有什么奇特的地方。\n",
    "嵌入:它只是围绕一个可以训练的NxE维张量的包装类，其中N是我们词汇表中的单词数，E是嵌入维数。这个表最初是随机的，但是它会随着时间而改变。当我们训练我们的网络时，梯度将一直反向传播到嵌入层，因此我们的词嵌入将被更新。我们将初始化嵌入层，我们将在我们的模型中使用我们的模型，然后我们在这里展示了一个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "thrown-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "monthly-furniture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.1258, -0.7275,  1.2096, -1.8804,  1.6865],\n",
       "         [-0.6010, -0.1004,  0.5889, -0.4271,  0.3562],\n",
       "         [-0.3893, -1.7048,  0.5934,  0.7237,  0.7924],\n",
       "         [ 0.0721,  0.0072, -0.3638,  1.1655, -0.7205],\n",
       "         [-0.9576, -0.0367,  0.2058, -0.9100, -0.4638],\n",
       "         [ 0.0304, -0.7949, -0.1166,  0.1753,  0.3865],\n",
       "         [-1.0422,  1.1801, -1.3600, -1.0125,  0.3692],\n",
       "         [ 0.6910, -0.8811,  0.0676, -1.5679, -1.5041],\n",
       "         [ 1.2847, -1.0386,  0.7956,  0.1647, -0.4481],\n",
       "         [ 0.8692,  1.2935,  0.3884,  2.4309,  0.5096],\n",
       "         [-2.2409, -0.8163,  0.7386,  1.1934,  0.1866],\n",
       "         [ 2.6747,  0.4610, -0.9643,  0.1758,  1.9913],\n",
       "         [ 1.6052, -0.2443, -0.8988, -0.1224,  0.7469],\n",
       "         [ 1.1621, -0.7050,  1.4881,  1.6442, -0.2078],\n",
       "         [-0.3992, -1.7097,  0.0521, -0.2182,  1.5734],\n",
       "         [-1.4902, -1.2717,  0.1680,  0.5448,  1.5934],\n",
       "         [ 0.3983,  1.8864, -0.3645, -0.5886, -0.2506],\n",
       "         [ 1.3114, -1.5087,  0.3137, -0.7421, -0.7476],\n",
       "         [ 0.8119, -1.1746,  0.3519, -1.6721, -0.6080],\n",
       "         [ 0.3574,  0.6240, -0.0117, -0.6228,  1.2123],\n",
       "         [-0.6745, -0.8101, -0.0461, -1.2177,  0.0394],\n",
       "         [ 0.0828,  0.2648,  0.1125,  0.0899, -1.4074],\n",
       "         [ 0.6425,  0.4750,  1.7716,  0.3201, -0.6662]], requires_grad=True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an embedding table for our words\n",
    "embedding_dim = 5\n",
    "embeds = nn.Embedding(len(vocabulary), embedding_dim)\n",
    "\n",
    "# Printing the parameters in our embedding table\n",
    "list(embeds.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-alignment",
   "metadata": {},
   "source": [
    "要将单词嵌入到词汇表中，我们所需要做的就是创建一个查找张量。查找张量就是一个张量包含我们要查找nn的指标。```nn.Embeding```需要一个长张量类型的索引张量，因此我们应该相应地创建我们的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "phantom-maria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量索引： tensor(15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.4902, -1.2717,  0.1680,  0.5448,  1.5934],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the embedding for the word Paris\n",
    "index = word_to_ix[\"paris\"]\n",
    "index_tensor = torch.tensor(index, dtype=torch.long)\n",
    "print(\"向量索引：\",index_tensor)\n",
    "paris_embed = embeds(index_tensor)\n",
    "paris_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "empirical-snake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量索引： [15, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4902, -1.2717,  0.1680,  0.5448,  1.5934],\n",
       "        [ 0.0721,  0.0072, -0.3638,  1.1655, -0.7205]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also get multiple embeddings at once\n",
    "index_paris = word_to_ix[\"paris\"]\n",
    "index_ankara = word_to_ix[\"ankara\"]\n",
    "indices = [index_paris, index_ankara]\n",
    "print(\"向量索引：\",indices)\n",
    "indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "embeddings = embeds(indices_tensor)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-uzbekistan",
   "metadata": {},
   "source": [
    "通常，我们将嵌入层定义为模型的一部分，您将在本子的后面部分中看到。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-complaint",
   "metadata": {},
   "source": [
    "### 1.3 批量句子\n",
    "我们已经在课堂上学习了分批。在进行更新之前，需要等待整个训练语料库的处理，这种方法时间成本很高。另一方面，在每个训练实例之后更新参数，会导致两次更新之间的损失不太稳定。为了解决这些问题，我们在对一批数据进行训练后再更新我们的参数。这使我们能够更好地估计总体损失的梯度。在本节中，我们将学习如何使用```torch.util.data.DataLoader```将数据组织成批。\n",
    "\n",
    "我们将按如下方式调用DataLoader类:```DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)```。batch_size参数确定每个批的示例数。在每个epoch中，我们将使用DataLoader遍历所有批次。默认情况下，批处理的顺序是确定的，但是我们可以通过将shuffle参数设置为True来要求DataLoader对批处理。通过这种方式，我们可以确保不会多次遇到不好的批次。\n",
    "\n",
    "如果提供，DataLoader将它准备的批处理传递给collate_fn。我们可以编写一个自定义函数来传递给collate_fn参数，以便打印关于批处理的统计信息或执行额外的处理。在本例中，我们将使用```collate_fn```来:\n",
    "\n",
    "    1. 填充我们用来训练的句子\n",
    "    2. 将训练示例中的单词转换为索引。\n",
    "    3. 填充训练的例子，使所有的句子和标签都有相同的长度。同样，我们也需要填充标签。这就产生了一个问题，因为在计算损失时，我们需要知道给定示例中的实际单词数。我们还将在传递给collate_fn参数的函数中跟踪这个数字。\n",
    "\n",
    "因为collate_fn函数的版本需要访问我们的word_to_ix字典(以便它可以将单词转换为索引)，所以我们将使用Python中的partial函数，它将所给的形参传递给传递给它的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cordless-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "def custom_collate_fn(batch, window_size, word_to_ix):\n",
    "  # Break our batch into the training examples (x) and labels (y)\n",
    "  # We are turning our x and y into tensors because nn.utils.rnn.pad_sequence\n",
    "  # method expects tensors. This is also useful since our model will be\n",
    "  # expecting tensor inputs. \n",
    "  x, y = zip(*batch)\n",
    "\n",
    "  # Now we need to window pad our training examples. We have already defined a \n",
    "  # function to handle window padding. We are including it here again so that\n",
    "  # everything is in one place.\n",
    "  def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "    window = [pad_token] * window_size\n",
    "    return window + sentence + window\n",
    "\n",
    "  # Pad the train examples.\n",
    "  x = [pad_window(s, window_size=window_size) for s in x]\n",
    "\n",
    "  # Now we need to turn words in our training examples to indices. We are\n",
    "  # copying the function defined earlier for the same reason as above.\n",
    "  def convert_tokens_to_indices(sentence, word_to_ix):\n",
    "    return [word_to_ix.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
    "\n",
    "  # Convert the train examples into indices.\n",
    "  x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n",
    "\n",
    "  # We will now pad the examples so that the lengths of all the example in \n",
    "  # one batch are the same, making it possible to do matrix operations. \n",
    "  # We set the batch_first parameter to True so that the returned matrix has \n",
    "  # the batch as the first dimension.\n",
    "  pad_token_ix = word_to_ix[\"<pad>\"]\n",
    "\n",
    "  # pad_sequence function expects the input to be a tensor, so we turn x into one\n",
    "  x = [torch.LongTensor(x_i) for x_i in x]\n",
    "  x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
    "\n",
    "  # We will also pad the labels. Before doing so, we will record the number \n",
    "  # of labels so that we know how many words existed in each example. \n",
    "  lengths = [len(label) for label in y]\n",
    "  lenghts = torch.LongTensor(lengths)\n",
    "\n",
    "  y = [torch.LongTensor(y_i) for y_i in y]\n",
    "  y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "  # We are now ready to return our variables. The order we return our variables\n",
    "  # here will match the order we read them in our training loop.\n",
    "  return x_padded, y_padded, lenghts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-heating",
   "metadata": {},
   "source": [
    "这个函数看起来很长，但实际上并不一定很长。看看下面的替代版本，我们删除了额外的函数声明和注释。|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "substantial-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _custom_collate_fn(batch, window_size, word_to_ix):\n",
    "  # Prepare the datapoints\n",
    "  x, y = zip(*batch)  \n",
    "  x = [pad_window(s, window_size=window_size) for s in x]\n",
    "  x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n",
    "\n",
    "  # Pad x so that all the examples in the batch have the same size\n",
    "  pad_token_ix = word_to_ix[\"<pad>\"]\n",
    "  x = [torch.LongTensor(x_i) for x_i in x]\n",
    "  x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
    "\n",
    "  # Pad y and record the length\n",
    "  lengths = [len(label) for label in y]\n",
    "  lenghts = torch.LongTensor(lengths)\n",
    "  y = [torch.LongTensor(y_i) for y_i in y]\n",
    "  y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "  return x_padded, y_padded, lenghts  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-determination",
   "metadata": {},
   "source": [
    "现在，我们可以就可以看到DataLoader如何运行的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "rubber-aside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 19, 16, 12,  8,  4,  0,  0],\n",
      "        [ 0,  0,  9,  7,  8, 18,  0,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 1, 0]])\n",
      "Batched Lengths:\n",
      "tensor([5, 4])\n",
      "\n",
      "Iteration 1\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 10, 13, 11, 17,  0,  0,  0],\n",
      "        [ 0,  0, 22,  2,  6, 20, 15,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([4, 5])\n",
      "\n",
      "Iteration 2\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([6])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate the DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# Go through one loop\n",
    "counter = 0\n",
    "for batched_x, batched_y, batched_lengths in loader:\n",
    "  print(f\"Iteration {counter}\")\n",
    "  print(\"Batched Input:\")\n",
    "  print(batched_x)\n",
    "  print(\"Batched Labels:\")\n",
    "  print(batched_y)\n",
    "  print(\"Batched Lengths:\")\n",
    "  print(batched_lengths)\n",
    "  print(\"\")\n",
    "  counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-electron",
   "metadata": {},
   "source": [
    "你在上面看到的批量输入张量将被传递到我们的模型中。另一方面，我们一开始就说我们的模型将是一个窗口分类器。根据当前的输入张量格式，我们在一个数据点中有一个句子中的所有单词。当我们将这个输入传递给我们的模型时，它需要为每个单词创建窗口，预测中心单词是否是每个窗口的位置，将预测放在一起并返回。\n",
    "\n",
    "我们可以避免这个问题，如果我们事先将数据分解到windows中。在这个例子中，我们将替换我们的模型于这种格式。\n",
    "\n",
    "假设我们的window_size是N，我们希望我们的模型对每2N+1个令牌做一个预测。也就是说，如果我们有一个包含9个标记的输入，并且window_size为2，那么我们希望我们的模型返回5个预测。这是有意义的，因为在我们用2个标记填充它之前，我们的输入也有5个标记!\n",
    "\n",
    "我们可以通过使用for循环来创建这些窗口，但是有一个更快的PyTorch替代方法，即展开(dimension, size, step)方法。我们可以使用下面的方法创建我们需要的窗口:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "satisfactory-resolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor: \n",
      "tensor([[ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0]])\n",
      "\n",
      "Windows: \n",
      "tensor([[[ 0,  0, 19,  5, 14],\n",
      "         [ 0, 19,  5, 14, 21],\n",
      "         [19,  5, 14, 21, 12],\n",
      "         [ 5, 14, 21, 12,  3],\n",
      "         [14, 21, 12,  3,  0],\n",
      "         [21, 12,  3,  0,  0]]])\n"
     ]
    }
   ],
   "source": [
    "# Print the original tensor\n",
    "print(f\"Original Tensor: \")\n",
    "print(batched_x)\n",
    "print(\"\")\n",
    "\n",
    "# Create the 2 * 2 + 1 chunks\n",
    "chunk = batched_x.unfold(1, window_size*2 + 1, 1)\n",
    "print(f\"Windows: \")\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-coordinate",
   "metadata": {},
   "source": [
    "## 2. 模型搭建\n",
    "现在我们已经准备好了数据，可以构建模型了。我们已经学会了如何写自定义nn。模块类。我们将在这里做同样的事情，并把我们目前学到的所有东西放在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "associate-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordWindowClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, hyperparameters, vocab_size, pad_ix=0):\n",
    "    super(WordWindowClassifier, self).__init__()\n",
    "    \n",
    "    \"\"\" Instance variables \"\"\"\n",
    "    self.window_size = hyperparameters[\"window_size\"]\n",
    "    self.embed_dim = hyperparameters[\"embed_dim\"]\n",
    "    self.hidden_dim = hyperparameters[\"hidden_dim\"]\n",
    "    self.freeze_embeddings = hyperparameters[\"freeze_embeddings\"]\n",
    "\n",
    "    \"\"\" Embedding Layer \n",
    "    Takes in a tensor containing embedding indices, and returns the \n",
    "    corresponding embeddings. The output is of dim \n",
    "    (number_of_indices * embedding_dim).\n",
    "\n",
    "    If freeze_embeddings is True, set the embedding layer parameters to be\n",
    "    non-trainable. This is useful if we only want the parameters other than the\n",
    "    embeddings parameters to change. \n",
    "\n",
    "    \"\"\"\n",
    "    self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_ix)\n",
    "    if self.freeze_embeddings:\n",
    "      self.embed_layer.weight.requires_grad = False\n",
    "\n",
    "    \"\"\" Hidden Layer\n",
    "    \"\"\"\n",
    "    full_window_size = 2 * window_size + 1\n",
    "    self.hidden_layer = nn.Sequential(\n",
    "      nn.Linear(full_window_size * self.embed_dim, self.hidden_dim), \n",
    "      nn.Tanh()\n",
    "    )\n",
    "\n",
    "    \"\"\" Output Layer\n",
    "    \"\"\"\n",
    "    self.output_layer = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    \"\"\" Probabilities \n",
    "    \"\"\"\n",
    "    self.probabilities = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    \"\"\"\n",
    "    Let B:= batch_size\n",
    "        L:= window-padded sentence length\n",
    "        D:= self.embed_dim\n",
    "        S:= self.window_size\n",
    "        H:= self.hidden_dim\n",
    "        \n",
    "    inputs: a (B, L) tensor of token indices\n",
    "    \"\"\"\n",
    "    B, L = inputs.size()\n",
    "\n",
    "    \"\"\"\n",
    "    Reshaping.\n",
    "    Takes in a (B, L) LongTensor\n",
    "    Outputs a (B, L~, S) LongTensor\n",
    "    \"\"\"\n",
    "    # Fist, get our word windows for each word in our input.\n",
    "    token_windows = inputs.unfold(1, 2 * self.window_size + 1, 1)\n",
    "    _, adjusted_length, _ = token_windows.size()\n",
    "\n",
    "    # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
    "    assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\n",
    "\n",
    "    \"\"\"\n",
    "    Embedding.\n",
    "    Takes in a torch.LongTensor of size (B, L~, S) \n",
    "    Outputs a (B, L~, S, D) FloatTensor.\n",
    "    \"\"\"\n",
    "    embedded_windows = self.embeds(token_windows)\n",
    "\n",
    "    \"\"\"\n",
    "    Reshaping.\n",
    "    Takes in a (B, L~, S, D) FloatTensor.\n",
    "    Resizes it into a (B, L~, S*D) FloatTensor.\n",
    "    -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
    "    \"\"\"\n",
    "    embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "\n",
    "    \"\"\"\n",
    "    Layer 1.\n",
    "    Takes in a (B, L~, S*D) FloatTensor.\n",
    "    Resizes it into a (B, L~, H) FloatTensor\n",
    "    \"\"\"\n",
    "    layer_1 = self.hidden_layer(embedded_windows)\n",
    "\n",
    "    \"\"\"\n",
    "    Layer 2\n",
    "    Takes in a (B, L~, H) FloatTensor.\n",
    "    Resizes it into a (B, L~, 1) FloatTensor.\n",
    "    \"\"\"\n",
    "    output = self.output_layer(layer_1)\n",
    "\n",
    "    \"\"\"\n",
    "    Softmax.\n",
    "    Takes in a (B, L~, 1) FloatTensor of unnormalized class scores.\n",
    "    Outputs a (B, L~, 1) FloatTensor of (log-)normalized class scores.\n",
    "    \"\"\"\n",
    "    output = self.probabilities(output)\n",
    "    output = output.view(B, -1)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-ghost",
   "metadata": {},
   "source": [
    "## 3. 训练\n",
    "现在我们已经准备好把所有东西放在一起了。让我们从准备数据和初始化模型开始。然后可以初始化优化器并定义损失函数。这一次，我们将定义自己的损失函数，而不是像以前那样使用预定义的损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "developmental-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate a DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize a model\n",
    "# It is useful to put all the model hyperparameters in a dictionary\n",
    "model_hyperparameters = {\n",
    "    \"batch_size\": 4,\n",
    "    \"window_size\": 2,\n",
    "    \"embed_dim\": 25,\n",
    "    \"hidden_dim\": 25,\n",
    "    \"freeze_embeddings\": False,\n",
    "}\n",
    "\n",
    "vocab_size = len(word_to_ix)\n",
    "model = WordWindowClassifier(model_hyperparameters, vocab_size)\n",
    "\n",
    "# Define an optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define a loss function, which computes to binary cross entropy loss\n",
    "def loss_function(batch_outputs, batch_labels, batch_lengths):   \n",
    "    # Calculate the loss for the whole batch\n",
    "    bceloss = nn.BCELoss()\n",
    "    loss = bceloss(batch_outputs, batch_labels.float())\n",
    "\n",
    "    # Rescale the loss. Remember that we have used lengths to store the \n",
    "    # number of words in each training example\n",
    "    loss = loss / batch_lengths.sum().float()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-feature",
   "metadata": {},
   "source": [
    "与前面的例子不同，这次我们将使用批处理，而不是在每个迭代中一次性将所有训练数据传递给模型。因此，在每个训练历元迭代中，我们遍历批次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "expanded-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will be called in every epoch\n",
    "def train_epoch(loss_function, optimizer, model, loader):\n",
    "  \n",
    "  # Keep track of the total loss for the batch\n",
    "  total_loss = 0\n",
    "  for batch_inputs, batch_labels, batch_lengths in loader:\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Run a forward pass\n",
    "    outputs = model.forward(batch_inputs)\n",
    "    # Compute the batch loss\n",
    "    loss = loss_function(outputs, batch_labels, batch_lengths)\n",
    "    # Calculate the gradients\n",
    "    loss.backward()\n",
    "    # Update the parameteres\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  return total_loss\n",
    "\n",
    "\n",
    "# Function containing our main training loop\n",
    "def train(loss_function, optimizer, model, loader, num_epochs=10000):\n",
    "\n",
    "  # Iterate through each epoch and call our train_epoch function\n",
    "  for epoch in range(num_epochs):\n",
    "    epoch_loss = train_epoch(loss_function, optimizer, model, loader)\n",
    "    if epoch % 100 == 0: print(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-boards",
   "metadata": {},
   "source": [
    "Let's start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "worldwide-trash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30812162160873413\n",
      "0.2404438816010952\n",
      "0.17627645283937454\n",
      "0.1325424425303936\n",
      "0.1190925408154726\n",
      "0.07673261314630508\n",
      "0.0607409942895174\n",
      "0.061209687031805515\n",
      "0.05008536949753761\n",
      "0.04399936180561781\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "train(loss_function, optimizer, model, loader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-burner",
   "metadata": {},
   "source": [
    "## 4. 预测\n",
    "让我们看看我们的模型在预测方面做得有多好。我们可以从创建测试数据开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "automotive-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [\"She comes from Paris\"]\n",
    "test_sentences = [s.lower().split() for s in test_corpus]\n",
    "test_labels = [[0, 0, 0, 1]]\n",
    "\n",
    "# Create a test loader\n",
    "test_data = list(zip(test_sentences, test_labels))\n",
    "batch_size = 1\n",
    "shuffle = False\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=2, word_to_ix=word_to_ix)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, \n",
    "                                           batch_size=1, \n",
    "                                           shuffle=False, \n",
    "                                           collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-division",
   "metadata": {},
   "source": [
    "让我们循环一下我们的测试示例，看看我们做得如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "international-enemy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 1]])\n",
      "tensor([[0.3551, 0.1014, 0.0432, 0.9527]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "for test_instance, labels, _ in test_loader:\n",
    "  outputs = model.forward(test_instance)\n",
    "  print(labels)\n",
    "  print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "basic-saudi",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3,4)\n",
    "a.size()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
